a1.sources=r1 r2
a1.channels=c1 c2
a1.sinks=k1 k2


a1.sources.r1.type=org.apache.flume.source.kafka.KafkaSource
a1.sources.r1.batchSize=5000
a1.sources.r1.batchDurationMillis=2000
a1.sources.r1.kafka.bootstrap.servers=hadoop102:9092,hadoop103:9092,hadoop104:9092
a1.sources.r1.kafka.topics=topic_save

a1.sources.r2.type=org.apache.flume.source.kafka.KafkaSource
a1.sources.r2.batchSize=5000
a1.sources.r2.batchDurationMillis=2000
a1.sources.r2.kafka.bootstrap.servers=hadoop102:9092,hadoop103:9092,hadoop104:9092
a1.sources.r2.kafka.topics=topic_analyze

a1.channels.c1.type=file
a1.channels.c1.checkpointDir=/opt/module/flume/checkpoint/behavior1
a1.channels.c1.dataDirs=/opt/module/flume/data/behavior1/
a1.channels.c1.maxFileSize=214635071
a1.channels.c1.capacity=1000000
a1.channels.c1.keep-alive=6

a1.channels.c2.type=file
a1.channels.c2.checkpointDir=/opt/module/flume/checkpoint/behavior2
a1.channels.c2.dataDirs=/opt/module/flume/data/behavior2/
a1.channels.c2.maxFileSize=214635071
a1.channels.c2.capacity=1000000
a1.channels.c2.keep-alive=6

a1.sinks.k1.type=hdfs
a1.sinks.k1.hdfs.path=/orgin_data/gmall/log/topic_save/%Y-%m-%d
a1.sinks.k1.hdfs.filePrefix=logsave-
a1.sinks.k1.hdfs.round=true
a1.sinks.k1.hdfs.roundValue=10
a1.sinks.k1.hdfs.roundUnit=second

a1.sinks.k2.type=hdfs
a1.sinks.k2.hdfs.path=/orgin_data/gmall/log/topic_analyze/%Y-%m-%d
a1.sinks.k2.hdfs.filePrefix=loganalyze-
a1.sinks.k2.hdfs.round=true
a1.sinks.k2.hdfs.roundValue=10
a1.sinks.k2.hdfs.roundUnit=second

a1.sinks.k1.hdfs.rollInterval=10
a1.sinks.k1.hdfs.rollSize=134217728
a1.sinks.k1.hdfs.rollCount=0

a1.sinks.k2.hdfs.rollInterval=10
a1.sinks.k2.hdfs.rollSize=134217728
a1.sinks.k2.hdfs.rollCount=0

a1.sinks.k1.hdfs.fileType=CompressedStream
a1.sinks.k2.hdfs.fileType=CompressedStream

a1.sinks.k1.hdfs.codeC=lzop
a1.sinks.k2.hdfs.codeC=lzop

a1.sources.r1.channels=c1
a1.sinks.k1.channel=c1

a1.sources.r2.channels=c2
a1.sinks.k2.channel=c2