a1.sources=r1
a1.channels=c1 c2

# configure sources
a1.sources.r1.type=TAILDIR
a1.sources.r1.positionFile=/opt/module/flume/test/log_position.json
a1.sources.r1.filegroups=f1
a1.sources.r1.filegroups.f1=/tmp/logs/app.+
a1.sources.r1.fileHeader=true
a1.sources.r1.channels=c1 c2

#interceptors
a1.sources.r1.interceptors=i1 i2
a1.sources.r1.interceptors.i1.type=com.nex.flume.interceptor.LogETLInterceptor$Builder

a1.sources.r1.interceptors.i2.type=com.nex.flume.interceptor.LogTypeInterceptor$Builder

a1.sources.r1.selector.type=multiplexing
a1.sources.r1.selector.header=topic
a1.sources.r1.selector.mapping.topic_save=c1
a1.sources.r1.selector.mapping.topic_analyze=c2

# configure channels
a1.channels.c1.type=org.apache.flume.channel.kafka.KafkaChannel
a1.channels.c1.kafka.bootstrap.servers=hadoop102:9092,hadoop103:9092,hadoop104:9092
a1.channels.c1.kafka.topic=topic_save
a1.channels.c1.parseAsFlumeEvent=false
a1.channels.c1.kafka.consumer.group.id=flume-consumer

a1.channels.c2.type=org.apache.flume.channel.kafka.KafkaChannel
a1.channels.c2.kafka.bootstrap.servers=hadoop102:9092,hadoop103:9092,hadoop104:9092
a1.channels.c2.kafka.topic=topic_analyze
a1.channels.c2.parseAsFlumeEvent=false
a1.channels.c2.kafka.consumer.group.id=flume-consumer
